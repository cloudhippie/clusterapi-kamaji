---
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: ${CLUSTER_NAME}
  labels:
    cluster: ${CLUSTER_NAME}
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 10.1.0.0/16
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
    kind: KamajiControlPlane
    name: ${CLUSTER_NAME}-control-plane
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: HetznerCluster
    name: ${CLUSTER_NAME}

...
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: HCloudMachineTemplate
metadata:
  name: ${CLUSTER_NAME}-worker
spec:
  template:
    spec:
      imageName: ubuntu-24.04
      placementGroupName: worker
      type: ${HCLOUD_WORKER_MACHINE_TYPE}

...
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: HCloudRemediationTemplate
metadata:
  name: worker-remediation-request
spec:
  template:
    spec:
      strategy:
        retryLimit: 1
        timeout: 180s
        type: Reboot

...
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: HetznerCluster
metadata:
  name: ${CLUSTER_NAME}
  annotations:
    capi.syself.com/allow-empty-control-plane-address: "true"
spec:
  controlPlaneLoadBalancer:
    enabled: false
  controlPlaneRegions:
    - fsn1
    - nbg1
    - hel1
  hcloudNetwork:
    enabled: true
  hcloudPlacementGroups:
    - name: worker
      type: spread
  hetznerSecretRef:
    key:
      hcloudToken: hcloud
      hetznerRobotPassword: robot-password
      hetznerRobotUser: robot-user
    name: hetzner
  sshKeys:
    hcloud:
      - name: ${SSH_KEY_NAME}
  skipCreatingHetznerSecretInWorkloadCluster: true

...
---
apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: KamajiControlPlane
metadata:
  name: ${CLUSTER_NAME}-control-plane
spec:
  replicas: ${CONTROL_PLANE_MACHINE_COUNT}
  version: ${KUBERNETES_VERSION}
  dataStoreName: default
  apiServer:
    extraArgs:
      - --oidc-client-id=kubernetes
      - --oidc-username-claim=preferred_username
      - '--oidc-username-prefix=oidc:'
      - --oidc-groups-claim=groups
      - '--oidc-groups-prefix=oidc:'
  controllerManager:
    extraArgs:
      - --cloud-provider=external
  scheduler: {}
  addons:
    konnectivity: {}
    kubeProxy: {}
    coreDNS:
      dnsServiceIPs:
        - 10.96.0.10
  kubelet:
    cgroupfs: systemd
    preferredAddressTypes:
      - ExternalIP
      - InternalIP
      - Hostname
  network:
    certSANs: []
    serviceType: LoadBalancer
    serviceAnnotations:
      external-dns.alpha.kubernetes.io/ttl: "1"
      load-balancer.hetzner.cloud/use-private-ip: "true"
      load-balancer.hetzner.cloud/disable-private-ingress: "true"
      load-balancer.hetzner.cloud/type: lb11
      load-balancer.hetzner.cloud/network-zone: eu-central
  deployment:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchExpressions:
                  - key: kamaji.clastix.io/name
                    operator: In
                    values:
                      - ${CLUSTER_NAME}

...
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: ${CLUSTER_NAME}-worker
spec:
  template:
    spec:
      files:
        - content: |
            net.ipv4.conf.lxc*.rp_filter = 0
          owner: root:root
          path: /etc/sysctl.d/99-cilium.conf
          permissions: "0744"
        - content: |
            overlay
            br_netfilter
          owner: root:root
          path: /etc/modules-load.d/crio.conf
          permissions: "0744"
        - content: |-
            net.bridge.bridge-nf-call-iptables = 1
            net.bridge.bridge-nf-call-ip6tables = 1
            net.ipv4.ip_forward = 1
            net.ipv6.conf.all.forwarding = 1
            net.ipv6.conf.all.accept_ra = 0
            net.ipv6.conf.default.accept_ra = 0
          owner: root:root
          path: /etc/sysctl.d/99-kubernetes-cri.conf
          permissions: "0744"
        - content: |
            vm.overcommit_memory=1
            kernel.panic=10
            kernel.panic_on_oops=1
          owner: root:root
          path: /etc/sysctl.d/99-kubelet.conf
          permissions: "0744"
        - content: |
            nameserver 1.1.1.1
            nameserver 1.0.0.1
            nameserver 2606:4700:4700::1111
          owner: root:root
          path: /etc/kubernetes/resolv.conf
          permissions: "0744"
        - content: |
            # Copyright The containerd Authors.
            #
            # Licensed under the Apache License, Version 2.0 (the "License");
            # you may not use this file except in compliance with the License.
            # You may obtain a copy of the License at
            #
            #     http://www.apache.org/licenses/LICENSE-2.0
            #
            # Unless required by applicable law or agreed to in writing, software
            # distributed under the License is distributed on an "AS IS" BASIS,
            # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
            # See the License for the specific language governing permissions and
            # limitations under the License.
            #
            # https://raw.githubusercontent.com/containerd/containerd/main/containerd.service

            [Unit]
            Description=containerd container runtime
            Documentation=https://containerd.io
            After=network.target local-fs.target dbus.service

            [Service]
            ExecStartPre=-/sbin/modprobe overlay
            ExecStart=/usr/local/bin/containerd

            Type=notify
            Delegate=yes
            KillMode=process
            Restart=always
            RestartSec=5

            # Having non-zero Limit*s causes performance problems due to accounting overhead
            # in the kernel. We recommend using cgroups to do container-local accounting.
            LimitNPROC=infinity
            LimitCORE=infinity

            # Comment TasksMax if your systemd version does not supports it.
            # Only systemd 226 and above support this version.
            TasksMax=infinity
            OOMScoreAdjust=-999

            [Install]
            WantedBy=multi-user.target
          owner: root:root
          path: /etc/systemd/system/containerd.service
          permissions: "0744"
        - path: /etc/sysctl.d/99-fsnotify.conf
          content: |-
            fs.inotify.max_user_instances = 8192
            fs.inotify.max_user_watches = 1048576
            fs.inotify.max_queued_events = 65536
      joinConfiguration:
        nodeRegistration:
          kubeletExtraArgs:
            anonymous-auth: "false"
            authentication-token-webhook: "true"
            authorization-mode: Webhook
            cloud-provider: external
            event-qps: "5"
            kubeconfig: /etc/kubernetes/kubelet.conf
            max-pods: "220"
            read-only-port: "0"
            resolv-conf: /etc/kubernetes/resolv.conf
            rotate-server-certificates: "true"
            tls-cipher-suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256
      preKubeadmCommands:
        - set -x
        - grep VERSION= /etc/os-release; uname -a
        - export CONTAINERD=1.7.26
        - export RUNC=1.2.5
        - export KUBERNETES_VERSION=$(echo ${KUBERNETES_VERSION} | sed 's/^v//')
        - export TRIMMED_KUBERNETES_VERSION=$(echo ${KUBERNETES_VERSION} | sed 's/^v//' | awk -F . '{print $1 "." $2}')
        - ARCH="$(dpkg --print-architecture)"
        - localectl set-locale LANG=en_US.UTF-8
        - localectl set-locale LANGUAGE=en_US.UTF-8
        - apt-get update -y
        - apt-get -y install at jq unzip wget socat mtr logrotate apt-transport-https
        - sed -i '/swap/d' /etc/fstab
        - swapoff -a
        - modprobe overlay && modprobe br_netfilter && sysctl --system
        - wget https://github.com/opencontainers/runc/releases/download/v$RUNC/runc.$ARCH
        - wget https://github.com/opencontainers/runc/releases/download/v$RUNC/runc.sha256sum
        - sha256sum --check --ignore-missing runc.sha256sum
        - install runc.$ARCH /usr/local/sbin/runc
        - rm -f runc.$ARCH runc.sha256sum
        - wget https://github.com/containerd/containerd/releases/download/v$CONTAINERD/containerd-$CONTAINERD-linux-$ARCH.tar.gz
        - wget https://github.com/containerd/containerd/releases/download/v$CONTAINERD/containerd-$CONTAINERD-linux-$ARCH.tar.gz.sha256sum
        - sha256sum --check containerd-$CONTAINERD-linux-$ARCH.tar.gz.sha256sum
        - tar -zxf containerd-$CONTAINERD-linux-$ARCH.tar.gz -C /usr/local
        - rm -f containerd-$CONTAINERD-linux-$ARCH.tar.gz containerd-$CONTAINERD-linux-$ARCH.tar.gz.sha256sum
        - mkdir -p /etc/containerd
        - containerd config default > /etc/containerd/config.toml
        - sed -i  "s/SystemdCgroup = false/SystemdCgroup = true/" /etc/containerd/config.toml
        - systemctl daemon-reload && systemctl enable containerd && systemctl start containerd
        - mkdir -p /etc/apt/keyrings/
        - curl -fsSL https://pkgs.k8s.io/core:/stable:/v$TRIMMED_KUBERNETES_VERSION/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
        - echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v$TRIMMED_KUBERNETES_VERSION/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
        - apt-get update
        - apt-get install -y kubelet="$KUBERNETES_VERSION-*" kubeadm="$KUBERNETES_VERSION-*" kubectl="$KUBERNETES_VERSION-*" bash-completion && apt-mark hold kubelet kubectl kubeadm && systemctl enable kubelet
        - kubeadm config images pull --kubernetes-version $KUBERNETES_VERSION
        - echo 'source <(kubectl completion bash)' >>/root/.bashrc
        - echo 'export KUBECONFIG=/etc/kubernetes/admin.conf' >>/root/.bashrc
        - apt-get -y autoremove && apt-get -y clean all
        - sysctl --system
        - echo "KUBELET_EXTRA_ARGS=--node-ip=$(curl -sSL -4 ifconfig.me | head -n1),$(curl -sSL -6 ifconfig.me | head -n1)" > /etc/default/kubelet

...
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  labels:
    nodepool: ${CLUSTER_NAME}-fsn1
    cluster: ${CLUSTER_NAME}
  name: ${CLUSTER_NAME}-fsn1
spec:
  clusterName: ${CLUSTER_NAME}
  replicas: ${WORKER_MACHINE_COUNT}
  selector:
    matchLabels: null
  template:
    metadata:
      labels:
        nodepool: ${CLUSTER_NAME}-fsn1
        cluster: ${CLUSTER_NAME}
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: ${CLUSTER_NAME}-worker
      clusterName: ${CLUSTER_NAME}
      failureDomain: fsn1
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: HCloudMachineTemplate
        name: ${CLUSTER_NAME}-worker
      version: ${KUBERNETES_VERSION}

...
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  labels:
    nodepool: ${CLUSTER_NAME}-hel1
    cluster: ${CLUSTER_NAME}
  name: ${CLUSTER_NAME}-hel1
spec:
  clusterName: ${CLUSTER_NAME}
  replicas: ${WORKER_MACHINE_COUNT}
  selector:
    matchLabels: null
  template:
    metadata:
      labels:
        nodepool: ${CLUSTER_NAME}-hel1
        cluster: ${CLUSTER_NAME}
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: ${CLUSTER_NAME}-worker
      clusterName: ${CLUSTER_NAME}
      failureDomain: hel1
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: HCloudMachineTemplate
        name: ${CLUSTER_NAME}-worker
      version: ${KUBERNETES_VERSION}

...
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  labels:
    nodepool: ${CLUSTER_NAME}-nbg1
    cluster: ${CLUSTER_NAME}
  name: ${CLUSTER_NAME}-nbg1
spec:
  clusterName: ${CLUSTER_NAME}
  replicas: ${WORKER_MACHINE_COUNT}
  selector:
    matchLabels: null
  template:
    metadata:
      labels:
        nodepool: ${CLUSTER_NAME}-nbg1
        cluster: ${CLUSTER_NAME}
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: ${CLUSTER_NAME}-worker
      clusterName: ${CLUSTER_NAME}
      failureDomain: nbg1
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: HCloudMachineTemplate
        name: ${CLUSTER_NAME}-worker
      version: ${KUBERNETES_VERSION}

...
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: ${CLUSTER_NAME}-worker-unhealthy-5m
spec:
  clusterName: ${CLUSTER_NAME}
  maxUnhealthy: 100%
  nodeStartupTimeout: 10m
  remediationTemplate:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: HCloudRemediationTemplate
    name: worker-remediation-request
  selector:
    matchLabels:
      cluster: ${CLUSTER_NAME}
  unhealthyConditions:
    - status: Unknown
      timeout: 180s
      type: Ready
    - status: "False"
      timeout: 180s
      type: Ready

...
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-${CLUSTER_NAME}-oidc
data:
  cluster-${CLUSTER_NAME}-oidc.yaml: |
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: oidc:cluster-${CLUSTER_NAME}
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: cluster-admin
    subjects:
      - apiGroup: rbac.authorization.k8s.io
        kind: Group
        name: oidc:cluster-${CLUSTER_NAME}

...
---
apiVersion: addons.cluster.x-k8s.io/v1beta1
kind: ClusterResourceSet
metadata:
  name: cluster-${CLUSTER_NAME}-oidc
spec:
  strategy: Reconcile
  clusterSelector:
    matchLabels:
      cluster: ${CLUSTER_NAME}
  resources:
    - name: cluster-${CLUSTER_NAME}-oidc
      kind: ConfigMap

...
